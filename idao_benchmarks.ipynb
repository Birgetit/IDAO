{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to facilitate your work with the data we provide several dumb benchmarks in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom metric is implemented here\n",
    "from scorer import scorer\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom dtypes for efficiency\n",
    "dtypes = {'id1': np.int16, 'id2': np.int16, 'id3': np.int16, 'user_id': np.int32, 'date': np.int16}\n",
    "\n",
    "train = pd.read_csv('train.csv.zip', dtype=dtypes)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select last 7 days to be validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_validation_start = train.date.max() - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_target(data, date_test_start):\n",
    "    '''\n",
    "        This function returns a dictionary of type {user: items_list}\n",
    "        Such that user viewed an item in testing period, \n",
    "        but did not view it within the last 3 weeks of train period.\n",
    "    '''\n",
    "    \n",
    "    test_mask = (data.date >= date_test_start) & (data.date < date_test_start + 7)\n",
    "    last_3weeks_mask = (data.date >= date_test_start - 21 + 1) & (data.date < date_test_start)\n",
    "    \n",
    "    # Items that used viewed during test period\n",
    "    items_test = data[test_mask].groupby('user_id').id3.apply(set)\n",
    "    \n",
    "    # Items, that user viewd in last 3 weeks\n",
    "    user_last_3weeks = data[last_3weeks_mask].groupby('user_id').id3.apply(set)\n",
    "    \n",
    "    # Get table, where for each `user_id` we have both items from test period and 3 weeks\n",
    "    joined = items_test.reset_index().merge(user_last_3weeks.reset_index(), on=['user_id'], how='left')\n",
    "    joined.set_index('user_id', inplace=True)\n",
    "    \n",
    "    # Remove the items, which the user viewed during last 3 weeks \n",
    "    target = {}\n",
    "    for user_id, (id3_x, id3_y) in joined.iterrows():   \n",
    "        items = id3_x if id3_y is np.nan else id3_x - id3_y\n",
    "        if items != set(): target.update({user_id: items})\n",
    "\n",
    "    return target\n",
    "\n",
    "# This function may take several minutes to finish\n",
    "y_val_dict = calculate_target(train, date_validation_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most simple benchmark: select 5% users at random and assign them items randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = train.id3.unique()\n",
    "users = train.user_id[train.date < date_validation_start].unique()\n",
    "num_users = len(users)\n",
    "\n",
    "# Select random users\n",
    "users_random_subset = np.random.choice(users, int(np.ceil(num_users * .05)), replace=False)\n",
    " \n",
    "# Select 5 random items for each user \n",
    "y_pred_dict = {user: np.random.choice(ids, 5) for user in users_random_subset}\n",
    "\n",
    "# Compute score \n",
    "score = scorer(y_val_dict, y_pred_dict, num_users)\n",
    "print (\"Random benchmark's score: %f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a simple benchmark using some machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask_train = train.date < date_validation_start - 7\n",
    "mask_test = (train.date < date_validation_start) & (train.date >= train.date.min() + 7)\n",
    "\n",
    "# For the sake of speed select only first 10k users to train on\n",
    "users_mask = train.user_id < 10000\n",
    "mask_train = mask_train & users_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feats(data):\n",
    "    '''\n",
    "        Builds sparse matrix using users' history.\n",
    "    '''\n",
    "    return scipy.sparse.coo_matrix(([1] * data.shape[0], (data.user_id, data.id3)), \n",
    "                                    shape =[data.user_id.max()+1, data.id3.max()+1]).tocsr()\n",
    "\n",
    "def get_target_matrix(X, target_dict):\n",
    "    '''\n",
    "        Builds sparse matrix using dictionary.\n",
    "    '''\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "\n",
    "    ks = []\n",
    "    for k in tqdm(range(X.user_id.max()+1)):\n",
    "        d = target_dict.get(k, [])\n",
    "        for y in d:\n",
    "            indices.append(y)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    return scipy.sparse.csr_matrix((data, indices, indptr), dtype=int, shape =[X.user_id.max()+1, X.id3.max()+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each user count how many items he viewed\n",
    "X_train = get_feats(train.loc[mask_train])\n",
    "X_test = get_feats(train.loc[mask_test])\n",
    "\n",
    "y_train_dict = calculate_target(train.loc[users_mask], date_validation_start - 7)\n",
    "y_train = get_target_matrix(train.loc[mask_train], y_train_dict)\n",
    "y_test = get_target_matrix(train.loc[mask_test], y_val_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For every id3 fit a separate Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit(i):\n",
    "    target = y_train[:, i].toarray().ravel()\n",
    "    \n",
    "    if target.mean() == 0:\n",
    "        return np.zeros((X_test.shape[0], )) - 1 \n",
    "    \n",
    "    d = LogisticRegression(max_iter=10)\n",
    "    d.fit(X_train, target)\n",
    "    return (d.predict_proba(X_test)[:, 1])\n",
    "\n",
    "preds = Parallel(n_jobs = 8, verbose=50)(delayed(fit)(i) for i in range(y_train.shape[1]))\n",
    "preds = np.vstack(preds).T\n",
    "\n",
    "# To reduce memory usage\n",
    "preds = preds.astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get item predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = int(np.ceil(num_users * 0.05))\n",
    "\n",
    "# Let's take not random users, but the ones who viewed a lot \n",
    "users = train.loc[mask_test].user_id.value_counts().index[:num]\n",
    "ans_inds =  np.argsort(preds[users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inds_dict =  {k: list(ans_inds[i, -5:]) for i,k in enumerate(users)}\n",
    "scorer(y_val_dict, test_inds_dict, num_users=num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to remove the id's, that user saw during last 3 weeks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each user find the categories, which we do not want to predict\n",
    "last_3weeks = train.loc[mask_test].loc[train.loc[mask_test].date >= train.loc[mask_test].date.max() - 21 + 1]\n",
    "y_not = last_3weeks.groupby('user_id').id3.apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = {}\n",
    "\n",
    "for u_idx, user in tqdm(enumerate(users)):\n",
    "    items_not = y_not.get(user, [])\n",
    "    items_pred = []\n",
    "    i = 1\n",
    "    while len(items_pred) < 5:\n",
    "        if not ans_inds[u_idx, -i] in items_not:\n",
    "            items_pred += [ans_inds[u_idx, -i]]\n",
    "    \n",
    "        i += 1\n",
    "    y_pred.update({user: items_pred})\n",
    "    \n",
    "print scorer(y_val_dict, y_pred, num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just very very dumb and simplistic benchmarks. It is possible to do much better on this task. Good luck!\n",
    "\n",
    "Finally, here is a snippet that will convert `y_pred` to (compressed) `.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame.from_records(y_pred).T.reset_index()\n",
    "y_pred_df.columns = ['user_id', 'id3_1', 'id3_2', 'id3_3', 'id3_4', 'id3_5']\n",
    "\n",
    "y_pred_df.to_csv('y_pred.csv', index=False)\n",
    "\n",
    "!rm y_pred.csv.zip; zip y_pred.csv.zip y_pred.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"./y_pred.csv.zip\">Link to download the submission from browser</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
